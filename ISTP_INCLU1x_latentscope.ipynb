{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Bennett's Data\n",
    "\n",
    "`INCLU1x IF Responses - ALL RUNS 041924.xlsx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "# nltk.download('punkt')  # Download the punkt tokenizer if you haven't already\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from latentscope_helper import latentscope_helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in and clean the data\n",
    "\n",
    "These next two cells only need to be run once.  (If rerunning this notebook, you can start after the next markdown cell.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this to True if you want to change the data the is used \n",
    "# (It takes some time to split the responses by sentences, so I will only do this once and then use that file late)\n",
    "read_in_original_data_file = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into sentences\n",
    "def split_into_sentences(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (read_in_original_data_file):\n",
    "    # read in the data\n",
    "    df = pd.read_excel(\"../../data/INCLU1x IF Responses - ALL RUNS 041924.xlsx\")\n",
    "    print(f\"length of original DataFrame = {len(df)}\")\n",
    "\n",
    "    # only take the columns we need and rename them to remove spaces and special characters\n",
    "    data_tmp = df[['ID#','Course Run','Student Response', 'Problem Code']].rename(columns = {'ID#':'ID','Course Run':'course_run','Student Response':'student_responses', 'Problem Code':'question_code'})\n",
    "\n",
    "\n",
    "    # remove extra newlines, etc.\n",
    "    data_tmp['student_responses'] = data_tmp['student_responses'].str.replace('\\n', ' ')\n",
    "\n",
    "    # this phrase appears a lot; we should remove it\n",
    "    data_tmp['student_responses'] = data_tmp['student_responses'].str.replace('Inclusive teaching is important to me because','') \n",
    "    \n",
    "    # remove rows with short answers (otherwise the sentence finder might choke -- not sure why)\n",
    "    n_min = 5\n",
    "    data_tmp = data_tmp[data_tmp['student_responses'].str.split().str.len().gt(n_min)]  \n",
    "\n",
    "    # get the unique questions, and save these to individual files\n",
    "    question_codes = data_tmp['question_code'].unique()\n",
    "\n",
    "    for qc in question_codes:\n",
    "\n",
    "        print(qc)\n",
    "        \n",
    "        data_use = data_tmp.loc[data_tmp['question_code'] == qc]\n",
    "\n",
    "        # save to .csv file\n",
    "        data_use.to_csv(\"../../data/INCLU1x_IF_Responses_-_ALL_RUNS_041924_\" + qc.replace(' ','_') + \"_cleaned.csv\", index=False)\n",
    "\n",
    "        # split into sentences\n",
    "        data = pd.DataFrame(columns=data_use.columns)\n",
    "        for index, row in data_use.iterrows():\n",
    "            # Split the response into sentences\n",
    "            sentences = split_into_sentences(row['student_responses'])\n",
    "            \n",
    "            # Create a new row for each sentence and append it to the new DataFrame\n",
    "            for sentence in sentences:\n",
    "                new_row = row.copy()\n",
    "                new_row['student_responses'] = sentence\n",
    "                data = data._append(new_row, ignore_index=True)\n",
    "\n",
    "        # remove rows with short answers (again)\n",
    "        n_min = 5\n",
    "        data = data[data['student_responses'].str.split().str.len().gt(n_min)]  \n",
    "\n",
    "        print(f\"length of new DataFrame (after cleaning and sentence splitting) = {len(data)}\")\n",
    "\n",
    "        # Save the new DataFrame to a new file (since this takes a while to run)\n",
    "        data.to_csv(\"../../data/INCLU1x_IF_Responses_-_ALL_RUNS_041924_\" + qc.replace(' ','_') + \"_cleaned_split_into_sentences.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and run `latent-scope` using my Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize my helper object with some parameters that will be common across all runs below\n",
    "worker = latentscope_helper(\n",
    "    latent_scope_dir = \"../../latent-scope_data\", # directory where the latentscope files are stored\n",
    "    text_column = \"student_responses\", # response column name from data_file\n",
    "    remove_old_files = True, # set this to True if you want to clean the latent-scope directories and start fresh\n",
    "    quick_clean = True, # set this to True if you want to remove every file in the directories, regardless of imin and imax\n",
    "    imin = 0, # minimum number of for files to search through (to remove)\n",
    "    imax = 50, # maximum number of scopes that it should search through\n",
    "    label_length = 10, # max length to tell the LLM to use when generating a given label (not always respected by the LLM!)\n",
    "    chat_model_instructions_before = \"Below is a list of items each starting with [item].  Each item is a response from a different person to a survey. These items all have a similar theme.  The list begins below.\", # string of text to provide the LLM as instructions before the list of cluster items is given\n",
    "    chat_model_instructions_after = \"That was the last item in the list.  Now return a concise label for the items in this list that describes the theme.  This label should not be fully verbatim text from any individual item.  Your label should contain no more than 10 words.\", # string of text to provide the LLM as instructions after the list of cluster items is given\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first pass just to get the embeddings (will run multiple times on the different data sets and embedding models)\n",
    "\n",
    "worker.run_embedding = True\n",
    "worker.run_umap = worker.run_label = worker.run_cluster = worker.save_scope = False\n",
    "\n",
    "\n",
    "#############\n",
    "# choose one data file\n",
    "#############\n",
    "# I am not going to split these into sentences here\n",
    "# fname = \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M0_IF_Reflection_Questions_cleaned.csv\"\n",
    "# fname = \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M1_IF_Reflection_Question_cleaned.csv\"\n",
    "# fname = \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M2_IF_Reflection_Question_cleaned.csv\"\n",
    "# fname = \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M3_IF_Reflection_Question_cleaned.csv\"\n",
    "# fname = \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M4_IF_Reflection_Question_cleaned.csv\"\n",
    "fname = \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M5_IF_Reflection_Question_cleaned.csv\"\n",
    "\n",
    "# these may need to be split into sentences, since they ask for multiple response (check with Bennett whether he wants to analyze these at all)\n",
    "# fname = \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M0_IF_Application_Question_cleaned.csv\"\n",
    "# fname = \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M1_IF_Application_Question_cleaned.csv\"\n",
    "# fname = \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M2_IF_Application_Question_cleaned.csv\"\n",
    "# fname = \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M3_IF_Application_Question_cleaned.csv\"\n",
    "# fname = \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M4_IF_Application_Question_cleaned.csv\"\n",
    "# fname = \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M5_IF_Application_Question_cleaned.csv\"\n",
    "\n",
    "#############\n",
    "\n",
    "data = pd.read_csv(\"../../data/\" + fname)\n",
    "\n",
    "\n",
    "#############\n",
    "# choose one embedding model\n",
    "#############\n",
    "worker.data = data\n",
    "worker.dataset_id = fname.replace('.csv','') + \"_UAE1024\"\n",
    "worker.embedding_model_id = \"transformers-WhereIsAI___UAE-Large-V1\"\n",
    "worker.embedding_n_dimensions = 1024\n",
    "\n",
    "# worker.data = data\n",
    "# worker.dataset_id = fname.replace('.csv','') + \"_bge1024\"\n",
    "# worker.embedding_model_id = \"transformers-BAAI___bge-large-en-v1.5\"\n",
    "# worker.embedding_n_dimensions = 1024\n",
    "\n",
    "# For some reason the kernel died for me on M0 (always at 51%, I also tried 256 dimensions with no luck)\n",
    "# worker.data = data\n",
    "# worker.dataset_id = fname.replace('.csv','') + \"_jina512\"\n",
    "# worker.embedding_model_id = \"transformers-jinaai___jina-embeddings-v2-small-en\"\n",
    "# worker.embedding_n_dimensions = 512  \n",
    "\n",
    "#############\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run embeddings\n",
    "worker.initialize_files_and_numbering()\n",
    "worker.initialize_latentscope()\n",
    "worker.run_latentscope()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metrics to assess the quality of this analysis\n",
    "\n",
    "Ideally, I would want to do this for a number of runs each changing some parameter and returning a different number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case I need to remove files (but keep the embeddings)\n",
    "worker.remove_old_files = True\n",
    "worker.quick_clean = True\n",
    "worker.initialize_files_and_numbering(dirs_to_remove = ['umaps', 'clusters', 'scopes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to True to run the metrics as a grid and False to draw random parameters for the metrics (between defined limits)\n",
    "rungrid = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "# %%capture magic to suppress output\n",
    "\n",
    "if rungrid:\n",
    "\n",
    "    # loop over parameters to check the resulting metrics\n",
    "    worker.suppress_latentscope_output = True\n",
    "    worker.suppress_helper_output = True\n",
    "    worker.remove_old_files = False\n",
    "    worker.initialize_files_and_numbering()\n",
    "\n",
    "    worker.embedding_number = '00001'\n",
    "    worker.run_embedding = worker.run_umap = worker.run_label = worker.run_cluster = worker.save_scope = False\n",
    "\n",
    "    umap_n_components = [2, 3]\n",
    "    umap_n_neighbors = [5, 10, 15, 20, 25, 30]\n",
    "    umap_min_dist = [0, 0.05, 0.1, 0.15]\n",
    "\n",
    "    cluster_min_samples = [5, 10, 15, 20, 25, 30]\n",
    "    cluster_selection_epsilon = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]\n",
    "\n",
    "    un = 1 # starting number for umap files\n",
    "    cn = 1 # starting number for the cluster files\n",
    "\n",
    "    cluster_file_numbers = []\n",
    "    umap_file_numbers = []\n",
    "\n",
    "    for u_nc in umap_n_components:\n",
    "        for u_nn in umap_n_neighbors:\n",
    "            for u_md in umap_min_dist:\n",
    "                print(f'umap_file_number = {un}, n_components = {u_nc}, n_neighbors = {u_nn}, min_dist = {u_md}')\n",
    "                worker.umap_n_components = u_nc\n",
    "                worker.umap_n_neighbors = u_nn\n",
    "                worker.umap_min_dist = u_md\n",
    "                worker.umap_number = str(un).zfill(5)\n",
    "                worker.run_umap = True\n",
    "                worker.run_cluster = False\n",
    "                worker.initialize_latentscope_filenames()\n",
    "                worker.run_latentscope()\n",
    "\n",
    "                \n",
    "                # cluster parameters\n",
    "                for c_ms in cluster_min_samples:\n",
    "                    for c_eps in cluster_selection_epsilon:\n",
    "                        print(f'cluster_file_number = {cn}, min_samples = {c_ms}, selection_epsilon = {c_eps}')\n",
    "                        worker.cluster_min_samples = c_ms\n",
    "                        worker.cluster_selection_epsilon = c_eps\n",
    "                        worker.cluster_number = str(cn).zfill(5)\n",
    "                        worker.run_umap = False\n",
    "                        worker.run_cluster = True\n",
    "                        worker.initialize_latentscope_filenames()\n",
    "                        worker.run_latentscope()\n",
    "            \n",
    "                        cluster_file_numbers.append(cn)\n",
    "                        umap_file_numbers.append(un)\n",
    "                        cn += 1\n",
    "                        \n",
    "                un += 1\n",
    "                print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not rungrid:\n",
    "\n",
    "    # draw parameters randomly, Ndraw_u*Ndraw_c times\n",
    " \n",
    "    Ndraw_u = 10 # number of different umap parameters\n",
    "    Ndraw_c = 50 # number of different HDBSCAN parameters (for each umap parameter set)\n",
    "\n",
    "    worker.suppress_latentscope_output = True\n",
    "    worker.suppress_helper_output = True\n",
    "    worker.remove_old_files = False\n",
    "    worker.initialize_files_and_numbering()\n",
    "\n",
    "    worker.embedding_number = '00001'\n",
    "    worker.run_embedding = worker.run_umap = worker.run_label = worker.run_cluster = worker.save_scope = False\n",
    "\n",
    "    umap_n_components_limits = [2, 5]\n",
    "    umap_n_neighbors_limits = [10, 100]\n",
    "    umap_min_dist_limits = [0, 0.15]\n",
    "\n",
    "    cluster_min_samples_limits = [5, 30]\n",
    "    cluster_selection_epsilon_limits = [0.01, 0.1]\n",
    "\n",
    "    un = 1 # starting number for umap files\n",
    "    cn = 1 # starting number for the cluster files\n",
    "\n",
    "    cluster_file_numbers = []\n",
    "    umap_file_numbers = []\n",
    "\n",
    "    for i in range(Ndraw_u):\n",
    "        u_nc = np.random.randint(low = umap_n_components_limits[0], high = umap_n_components_limits[1] + 1)\n",
    "        u_nn = np.random.randint(low = umap_n_neighbors_limits[0], high = umap_n_neighbors_limits[1] + 1)\n",
    "        u_md = np.random.random()*(umap_min_dist_limits[1] - umap_min_dist_limits[0]) + umap_min_dist_limits[0]\n",
    "\n",
    "        print(f'umap_file_number = {un}, n_components = {u_nc}, n_neighbors = {u_nn}, min_dist = {u_md}')\n",
    "        worker.umap_n_components = u_nc\n",
    "        worker.umap_n_neighbors = u_nn\n",
    "        worker.umap_min_dist = u_md\n",
    "        worker.umap_number = str(un).zfill(5)\n",
    "        worker.run_umap = True\n",
    "        worker.run_cluster = False\n",
    "        worker.initialize_latentscope_filenames()\n",
    "        worker.run_latentscope()\n",
    "        \n",
    "        for j in range(Ndraw_c):\n",
    "            c_ms = np.random.randint(low = cluster_min_samples_limits[0], high = cluster_min_samples_limits[1] + 1)\n",
    "            c_eps = np.random.random()*(cluster_selection_epsilon_limits[1] - cluster_selection_epsilon_limits[0]) + cluster_selection_epsilon_limits[0]\n",
    "            \n",
    "            print(f'cluster_file_number = {cn}, min_samples = {c_ms}, selection_epsilon = {c_eps}')\n",
    "            worker.cluster_min_samples = c_ms\n",
    "            worker.cluster_selection_epsilon = c_eps\n",
    "            worker.cluster_number = str(cn).zfill(5)\n",
    "            worker.run_umap = False\n",
    "            worker.run_cluster = True\n",
    "            worker.initialize_latentscope_filenames()\n",
    "            worker.run_latentscope()\n",
    "\n",
    "            cluster_file_numbers.append(cn)\n",
    "            umap_file_numbers.append(un)\n",
    "            cn += 1\n",
    "                        \n",
    "        un += 1\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "inertia = []\n",
    "sc = []\n",
    "ch = []\n",
    "db = []\n",
    "n_neighbors = []\n",
    "min_dist = []\n",
    "n_components = []\n",
    "min_samples = []\n",
    "selection_epsilon = []\n",
    "n_clusters = []\n",
    "for (un,cn) in zip(umap_file_numbers, cluster_file_numbers):\n",
    "    print(un, cn)\n",
    "    m = worker.calculate_metrics(embedding_number = '00001', umap_number = str(un).zfill(5), cluster_number = str(cn).zfill(5))\n",
    "    metrics.append(m)\n",
    "    inertia.append(m['inertia'])\n",
    "    sc.append(m['silhouette_coefficient'])\n",
    "    ch.append(m['calinski_harabasz_index'])\n",
    "    db.append(m['davies_bouldin_index'])\n",
    "    n_components.append(m['umap_info']['n_components'])\n",
    "    n_neighbors.append(m['umap_info']['neighbors'])\n",
    "    min_dist.append(m['umap_info']['min_dist'])\n",
    "    min_samples.append(m['cluster_info']['min_samples'])\n",
    "    selection_epsilon.append(m['cluster_info']['cluster_selection_epsilon'])\n",
    "    n_clusters.append(m['cluster_info']['n_clusters'])\n",
    "df = pd.DataFrame()\n",
    "df['umap_file_number'] = umap_file_numbers\n",
    "df['cluster_file_number'] = cluster_file_numbers\n",
    "df['umap_n_components'] = n_components\n",
    "df['umap_n_neighbors'] = n_neighbors\n",
    "df['umap_min_dist'] = min_dist\n",
    "df['cluster_min_samples'] = min_samples\n",
    "df['cluster_selection_epsilon'] = selection_epsilon\n",
    "df['n_clusters'] = n_clusters\n",
    "df['inertia'] = inertia\n",
    "df['silhouette_coefficient'] = sc\n",
    "df['calinski_harabasz_index'] = ch\n",
    "df['davies_bouldin_index'] = db\n",
    "\n",
    "\n",
    "if rungrid:\n",
    "    df.to_csv(os.path.join('tables', worker.dataset_id + '_metrics_grid.csv'), index = False)\n",
    "else:\n",
    "    df.to_csv(os.path.join('tables', worker.dataset_id + '_metrics_MC.csv'), index = False)\n",
    "\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm still having trouble resetting the plotting backend given the suppression above \n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rungrid:\n",
    "    dfm = pd.read_csv(os.path.join('tables', worker.dataset_id + '_metrics_grid.csv'))\n",
    "else:\n",
    "    dfm = pd.read_csv(os.path.join('tables', worker.dataset_id + '_metrics_MC.csv'))\n",
    "\n",
    "dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_plot = dfm.columns[2:].tolist()\n",
    "g = sns.pairplot(dfm[cols_to_plot], corner = True, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 0.15\n",
    "dfm['combined_metric'] = minmax_scale(dfm['inertia']) + minmax_scale(dfm['silhouette_coefficient']) + minmax_scale(dfm['calinski_harabasz_index']) + (1. - minmax_scale(dfm['davies_bouldin_index']))\n",
    "\n",
    "dfm_best = dfm.loc[(dfm['n_clusters'] > 2) & (dfm['n_clusters'] < 30) & (dfm['silhouette_coefficient'] > (1. - f)*dfm['silhouette_coefficient'].max()) & (dfm['calinski_harabasz_index'] > (1. - f)*dfm['calinski_harabasz_index'].max()) & (dfm['davies_bouldin_index'] < (1. + f)*dfm['davies_bouldin_index'].min())]\n",
    "\n",
    "dfm_best.sort_values(by=[\"n_clusters\", \"combined_metric\"], ascending = [True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vars = ['umap_n_components','umap_n_neighbors','umap_min_dist','cluster_min_samples','cluster_selection_epsilon','n_clusters']\n",
    "y_vars = ['silhouette_coefficient','calinski_harabasz_index','davies_bouldin_index', 'inertia']\n",
    "g = sns.PairGrid(dfm, x_vars = x_vars, y_vars = y_vars, hue = 'n_clusters', palette = 'viridis')\n",
    "g.map(sns.scatterplot)\n",
    "#g.map_diag(sns.histplot)\n",
    "\n",
    "# I played around with the loc statement to isolate what appears to be the best clusters\n",
    "# (and asked ChatGPT how to create this overlay)\n",
    "# dfm_best = df.loc[(dfm['n_clusters'] > 5) & (dfm['n_clusters'] < 500) & (dfm['silhouette_coefficient'] > -0.2) & (dfm['calinski_harabasz_index'] > 6000) & (dfm['davies_bouldin_index'] < 5)]\n",
    "def custom_scatter(x, y, **kwargs):\n",
    "    sns.scatterplot(x=x, y=y, **kwargs)\n",
    "    sns.scatterplot(data=dfm_best, x=x.name, y=y.name, marker='o', color='black', edgecolor='black', facecolors='none', s=100)\n",
    "g.map(custom_scatter)\n",
    "\n",
    "if rungrid:\n",
    "    g.savefig(os.path.join('plots', worker.dataset_id + '_metric_grid.png'), bbox_inches = 'tight') \n",
    "else:\n",
    "    g.savefig(os.path.join('plots', worker.dataset_id + '_metric_MC.png'), bbox_inches = 'tight') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm_best.sort_values(by=[\"n_clusters\", \"combined_metric\"], ascending = [True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker.dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick one of these to create labels and save the scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker.suppress_latentscope_output = False\n",
    "worker.suppress_helper_output = False\n",
    "worker.remove_old_files = False\n",
    "worker.embedding_number = '00001'\n",
    "\n",
    "worker.run_embedding = worker.run_umap = worker.run_label = worker.run_cluster = False\n",
    "worker.run_label = worker.save_scope = True\n",
    "\n",
    "# I need to run the combinations (after deciding on bge and UMAP params)\n",
    "\n",
    "###################\n",
    "# choose between data sets\n",
    "###################\n",
    "\n",
    "###################\n",
    "# M0\n",
    "# worker.dataset_id = 'INCLU1x_IF_Responses_-_ALL_RUNS_041924_M0_IF_Reflection_Questions_cleaned_UAE1024'\n",
    "# worker.umap_number = '00001'\n",
    "# worker.cluster_number = '00017'\n",
    "\n",
    "# worker.dataset_id = 'INCLU1x_IF_Responses_-_ALL_RUNS_041924_M0_IF_Reflection_Questions_cleaned_bge1024'\n",
    "# worker.umap_number = '00010'\n",
    "# worker.cluster_number = '00462'\n",
    "\n",
    "###################\n",
    "# M1\n",
    "# worker.dataset_id = 'INCLU1x_IF_Responses_-_ALL_RUNS_041924_M1_IF_Reflection_Questions_cleaned_UAE1024'\n",
    "# worker.umap_number = '00008'\n",
    "# worker.cluster_number = '00355'\n",
    "\n",
    "# worker.dataset_id = 'INCLU1x_IF_Responses_-_ALL_RUNS_041924_M1_IF_Reflection_Questions_cleaned_bge1024'\n",
    "# worker.umap_number = '00001'\n",
    "# worker.cluster_number = '00032'\n",
    "\n",
    "# worker.dataset_id = 'INCLU1x_IF_Responses_-_ALL_RUNS_041924_M1_IF_Reflection_Questions_cleaned_jina512'\n",
    "# worker.umap_number = '00005'\n",
    "# worker.cluster_number = '00219'\n",
    "\n",
    "\n",
    "###################\n",
    "# M2\n",
    "# worker.dataset_id = 'INCLU1x_IF_Responses_-_ALL_RUNS_041924_M2_IF_Reflection_Questions_cleaned_UAE1024'\n",
    "# worker.umap_number = '00006'\n",
    "# worker.cluster_number = '00282'\n",
    "\n",
    "# worker.dataset_id = 'INCLU1x_IF_Responses_-_ALL_RUNS_041924_M2_IF_Reflection_Questions_cleaned_bge1024'\n",
    "# worker.umap_number = '00005'\n",
    "# worker.cluster_number = '00209'\n",
    "\n",
    "# worker.dataset_id = 'INCLU1x_IF_Responses_-_ALL_RUNS_041924_M2_IF_Reflection_Questions_cleaned_jina512'\n",
    "# worker.umap_number = '00001'\n",
    "# worker.cluster_number = '00008'\n",
    "\n",
    "\n",
    "###################\n",
    "# M3\n",
    "# worker.dataset_id = 'INCLU1x_IF_Responses_-_ALL_RUNS_041924_M3_IF_Reflection_Questions_cleaned_UAE1024'\n",
    "# worker.umap_number = '00001'\n",
    "# worker.cluster_number = '00038'\n",
    "\n",
    "# worker.dataset_id = 'INCLU1x_IF_Responses_-_ALL_RUNS_041924_M3_IF_Reflection_Questions_cleaned_bge1024'\n",
    "# worker.umap_number = '00001'\n",
    "# worker.cluster_number = '00003'\n",
    "\n",
    "# worker.dataset_id = 'INCLU1x_IF_Responses_-_ALL_RUNS_041924_M3_IF_Reflection_Questions_cleaned_jina512'\n",
    "# worker.umap_number = '00001'\n",
    "# worker.cluster_number = '00017'\n",
    "\n",
    "\n",
    "###################\n",
    "# M4\n",
    "# worker.dataset_id = 'INCLU1x_IF_Responses_-_ALL_RUNS_041924_M4_IF_Reflection_Questions_cleaned_UAE1024'\n",
    "# worker.umap_number = '00006'\n",
    "# worker.cluster_number = '00251'\n",
    "\n",
    "# worker.dataset_id = 'INCLU1x_IF_Responses_-_ALL_RUNS_041924_M4_IF_Reflection_Questions_cleaned_bge1024'\n",
    "# worker.umap_number = '00001'\n",
    "# worker.cluster_number = '00018'\n",
    "\n",
    "# worker.dataset_id = 'INCLU1x_IF_Responses_-_ALL_RUNS_041924_M4_IF_Reflection_Questions_cleaned_jina512'\n",
    "# worker.umap_number = '00001'\n",
    "# worker.cluster_number = '00006'\n",
    "\n",
    "\n",
    "###################\n",
    "# M5\n",
    "# worker.dataset_id = 'INCLU1x_IF_Responses_-_ALL_RUNS_041924_M5_IF_Reflection_Questions_cleaned_UAE1024'\n",
    "worker.umap_number = '00006'\n",
    "worker.cluster_number = '00277'\n",
    "\n",
    "# worker.dataset_id = 'INCLU1x_IF_Responses_-_ALL_RUNS_041924_M5_IF_Reflection_Questions_cleaned_bge1024'\n",
    "# worker.umap_number = '00009'\n",
    "# worker.cluster_number = '00403'\n",
    "\n",
    "# worker.dataset_id = 'INCLU1x_IF_Responses_-_ALL_RUNS_041924_M5_IF_Reflection_Questions_cleaned_jina512'\n",
    "# worker.umap_number = '00001'\n",
    "# worker.cluster_number = '00001'\n",
    "\n",
    "###################\n",
    "# choose the LLM\n",
    "###################\n",
    "\n",
    "worker.chat_model_id = 'transformers-HuggingFaceH4___zephyr-7b-beta'\n",
    "worker.chat_file_label = 'zephyr'\n",
    "worker.scope_number = '00001'\n",
    "worker.label_number = '00001'\n",
    "\n",
    "# worker.chat_model_id = 'transformers-TinyLlama___TinyLlama-1.1B-Chat-v1.0'\n",
    "# worker.chat_file_label = 'tinyllama'\n",
    "# worker.scope_number = '00002'\n",
    "# worker.label_number = '00002'\n",
    "\n",
    "worker.initialize_files_and_numbering()\n",
    "worker.initialize_latentscope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "worker.run_latentscope()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print and save the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to plot a specific scope number, you can define it here (and you don't need to actually run latentscope in the previous cell)\n",
    "# worker.remove_old_files = False\n",
    "# worker.scope_number = '00001'\n",
    "# worker.initialize_files_and_numbering()\n",
    "# worker.initialize_latentscope()\n",
    "rungrid = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the labels\n",
    "worker.print_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bar chart of the labels\n",
    "if (rungrid):\n",
    "    f, ax = worker.create_bar_chart(filename = os.path.join('plots', worker.dataset_id + '_' + worker.chat_file_label  + '_bar_grid.png'))\n",
    "else:\n",
    "    f, ax = worker.create_bar_chart(filename = os.path.join('plots', worker.dataset_id + '_' + worker.chat_file_label + '_bar_MC.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an Excel workbook to review the results\n",
    "# The first sheet will have the raw data.  \n",
    "# The second sheet will have a map between cluster label and sheet name.  \n",
    "# Subsequent sheets will be one per cluster containing the cluster data.\n",
    "data_raw = data.copy()\n",
    "\n",
    "if (rungrid):\n",
    "    worker.create_excel_workbook(data_raw, os.path.join('tables', worker.dataset_id + '_clusters_' + worker.chat_file_label + '_grid.xlsx'))\n",
    "else:\n",
    "    worker.create_excel_workbook(data_raw, os.path.join('tables', worker.dataset_id + '_clusters_' + worker.chat_file_label + '_MC.xlsx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the server to investigate and visualize these results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import latentscope as ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls.serve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available models can be printed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a list of possible embedding models\n",
    "# [m[\"id\"] for m in ls.models.get_embedding_model_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a list of available LLMS for labelling the clusters\n",
    "# [m[\"id\"] for m in ls.models.get_chat_model_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
