{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try sending answers directly to the OpenAI API on Azure\n",
    "\n",
    "using GPT-4o, which has a context limit of 128k\n",
    "\n",
    "working in my conda env \n",
    "```\n",
    "conda create --name openai-wsl python=3.10 numpy pandas jupyter matplotlib python-dotenv\n",
    "conda activate openai-wsl\n",
    "pip install --upgrade openai \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the APIU key, endpoint, and set other necessary inof\n",
    "load_dotenv()\n",
    "my_azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "my_api_key  = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "my_api_version = \"2024-02-15-preview\"\n",
    "deployment_name = \"test_GPT_4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "froots = [\n",
    "    \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M0_IF_Reflection_Questions_cleaned\",\n",
    "    \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M1_IF_Reflection_Question_cleaned\",\n",
    "    \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M2_IF_Reflection_Question_cleaned\",\n",
    "    \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M3_IF_Reflection_Question_cleaned\",\n",
    "    \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M4_IF_Reflection_Question_cleaned\",\n",
    "    \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M5_IF_Reflection_Question_cleaned\" ,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of words in each file\n",
    "# from OpenAI: https://platform.openai.com/tokenizer \"A helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughly Â¾ of a word (so 100 tokens ~= 75 words).\"\n",
    "# the largest GPT model has 128K tokens context limit (so it won't be able to ingest everything)\n",
    "for f in froots:\n",
    "    df = pd.read_csv(\"../../data/\" + f + \".csv\")\n",
    "    all_answers = df['student_responses'].str.cat(sep=' ').split(' ')\n",
    "    print(f, len(all_answers)*4/3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions_before = \"Below is a list of items each starting with [item]. I will want you to summarize this list.\"\n",
    "instructions_after = \"That was the last item in the list.  Now summarize these items with a new list of 10 or less unique themes.  Do not repeat any item from above verbatim in your themes.  Each theme should be only one short sentence.  Only return the short one-sentence themes.\"\n",
    "\n",
    "# define the openAI client \n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint = my_azure_endpoint,\n",
    "    api_key = my_api_key,\n",
    "    api_version = my_api_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LLM_input_max_index(input_list, max_tokens = 1048, start_index = 0, system_text = '', extra_tokens_per_item = 0):\n",
    "    # determine how many lines I can feed into the LLM\n",
    "\n",
    "    n_tokens = len(system_text.split())*4/3.\n",
    "    for index, item in enumerate(input_list):\n",
    "        if (index > start_index):\n",
    "            n_tokens_item = len(item.split())*4/3.\n",
    "            n_tokens += n_tokens_item + extra_tokens_per_item\n",
    "            if (n_tokens > max_tokens):\n",
    "                return index - 1\n",
    "\n",
    "    return index\n",
    "\n",
    "def run_openai_api(froot, max_tokens, start_index = 0):\n",
    "    df = pd.read_csv(\"../../data/\" + froot + \".csv\")\n",
    "    responses = df['student_responses'].to_list()\n",
    "\n",
    "    # determine how many answers I can fit in one call\n",
    "    end_index = get_LLM_input_max_index(responses, start_index = start_index, max_tokens = max_tokens, system_text = instructions_before + \" \" + instructions_after, extra_tokens_per_item = 1)\n",
    "\n",
    "    # define the message to the LLM\n",
    "    input_list = ''\n",
    "    for item in responses[start_index:(end_index - start_index)]:\n",
    "        input_list += '[item] ' + item + '\\n'\n",
    "    messages=[\n",
    "        {\"role\":\"system\", \"content\":instructions_before},\n",
    "        {\"role\":\"user\", \"content\": input_list},\n",
    "        {\"role\":\"system\", \"content\":instructions_after}\n",
    "    ]\n",
    " \n",
    "    # Get the response from the client\n",
    "    response = client.chat.completions.create(\n",
    "        model = deployment_name,\n",
    "        messages = messages\n",
    "    )\n",
    "\n",
    "    # print the result\n",
    "    out_message = response.choices[0].message.content\n",
    "    return len(responses), end_index, input_list, out_message\n",
    "\n",
    "def write_summary_file(froot, end_index, n_answers, output):\n",
    "    with open(os.path.join('tables','openAI',froot + \"_GPT4o.txt\"), 'w') as f:\n",
    "        f.write('file root : ' + froot + '\\n\\n')\n",
    "        f.write('total number of answers in file : ' + str(n_answers) + '\\n')\n",
    "        f.write('number of answers included : ' +  str(end_index) + '\\n\\n')\n",
    "        f.write('OpenAI GTP-4o response :\\n')\n",
    "        f.write(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 128000\n",
    "\n",
    "for i,froot in enumerate(froots):\n",
    "    if (i > 2):\n",
    "        print(froot)\n",
    "\n",
    "        n_answers, end_index, input, output = run_openai_api(froot, max_tokens)\n",
    "        write_summary_file(froot, end_index, n_answers, output)\n",
    "        print(output)\n",
    "        \n",
    "        print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
