{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Bennett's Data\n",
    "\n",
    "`ITP_CourseArtifacts_June 2021_END_of_Course_DeIDENTIFIED.xlsx` using the `Course Meta App` sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "# nltk.download('punkt')  # Download the punkt tokenizer if you haven't already\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import latentscope as ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in and clean the data\n",
    "\n",
    "These next two cells only need to be run once.  (If rerunning this notebook, you can start after the next markdown cell.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this to True if you want to change the data the is used \n",
    "read_in_original_data_file = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into sentences\n",
    "def split_into_sentences(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (read_in_original_data_file):\n",
    "    # read in the data\n",
    "    df = pd.read_excel(\"../../data/ITP_CourseArtifacts_June 2021_END_of_Course_DeIDENTIFIED.xlsx\", sheet_name = \"Course Meta App\")\n",
    "    print(f\"length of original DataFrame = {len(df)}\")\n",
    "\n",
    "    # save the question\n",
    "    question = df.columns[1]\n",
    "    # only take the columns we need and rename them to remove spaces and special characters\n",
    "    data_tmp = df.rename(columns = {question:'student_responses'})\n",
    "\n",
    "    # remove extra newlines and special characters \n",
    "    #(some special characters were apparently converted to ascii before creating the xlsx file and remain as e.g., \\u2019 ... )\n",
    "    data_tmp['student_responses'] = data_tmp['student_responses'].str.replace('\\n', ' ').replace(r'[^\\w\\s]', '').str.replace(r'[^\\x00-\\x7F]', '')\n",
    "\n",
    "    # remove rows with short answers (otherwise the sentence finder might choke -- not sure why)\n",
    "    n_min = 5\n",
    "    data_tmp = data_tmp[data_tmp['student_responses'].str.split().str.len().gt(n_min)]  \n",
    "\n",
    "    # save to .csv file\n",
    "    data_tmp.to_csv(\"../../data/ITP_CourseArtifacts_June_2021_END_of_Course_DeIDENTIFIED_Course_Meta_App_cleaned.csv\", index=False)\n",
    "\n",
    "    # split into sentences\n",
    "    data = pd.DataFrame(columns=data_tmp.columns)\n",
    "    for index, row in data_tmp.iterrows():\n",
    "        # Split the response into sentences\n",
    "        sentences = split_into_sentences(row['student_responses'])\n",
    "        \n",
    "        # Create a new row for each sentence and append it to the new DataFrame\n",
    "        for sentence in sentences:\n",
    "            new_row = row.copy()\n",
    "            new_row['student_responses'] = sentence\n",
    "            data = data._append(new_row, ignore_index=True)\n",
    "\n",
    "    # remove rows with short answers (again)\n",
    "    n_min = 5\n",
    "    data = data[data['student_responses'].str.split().str.len().gt(n_min)]  \n",
    "\n",
    "    print(f\"length of new DataFrame (after cleaning and sentence splitting) = {len(data)}\")\n",
    "\n",
    "    # Save the new DataFrame to a new file (since this takes a while to run)\n",
    "    data.to_csv(\"../../data/ITP_CourseArtifacts_June_2021_END_of_Course_DeIDENTIFIED_Course_Meta_App_cleaned_split_into_sentences.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This question has multiple components; so definitely best to take the version with split sentences\n",
    "data = pd.read_csv(\"../../data/ITP_CourseArtifacts_June_2021_END_of_Course_DeIDENTIFIED_Course_Meta_App_cleaned_split_into_sentences.csv\")\n",
    "\n",
    "\n",
    "data#['student_responses']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize `latent-scope`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the directory where all the latent-scope results are stored (this can be the same directory for multiple projects)\n",
    "latent_scope_dir = \"../../latent-scope_data\"\n",
    "\n",
    "# data set name, for the sub-directory name within latent_scope_dir for this project\n",
    "dataset_id = \"ITP_CourseArtifacts_June_2021_END_of_Course_DeIDENTIFIED_Course_Meta_App\"\n",
    "\n",
    "ls.init(latent_scope_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the scope number to use (can keep as 'new', unless you want to rerun something)\n",
    "# this string is appended to the end of each file saved for each step in latent-scope,\n",
    "# use 'new' if you want to create a new scope \n",
    "scope_number = 'new' \n",
    "\n",
    "########################\n",
    "# REMOVE PREVIOUS FILES?... BEWARE\n",
    "remove_old_files = True # set this to True if you want to clean the latent-scope directories and start fresh\n",
    "imax = 50 # maximum number of scopes that it should search through\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE PREVIOUS FILES?... BEWARE\n",
    "if (remove_old_files):\n",
    "    scope_number = '001'\n",
    "    for i in range(imax):\n",
    "        for d in ['clusters', 'embeddings', 'umaps', 'scopes']:\n",
    "            for f in glob.glob(os.path.join(latent_scope_dir, dataset_id, d, '*'+str(i).zfill(3)+'*')):\n",
    "                print(\"removing : \",f)\n",
    "                os.remove(f)\n",
    "else:\n",
    "    if (scope_number ==  'new'):\n",
    "        nums = []\n",
    "        fls = []\n",
    "        for f in glob.glob(os.path.join(latent_scope_dir, dataset_id, 'embeddings', '*.json')):\n",
    "            fls.append(f)\n",
    "            x = re.split('-|\\.', f)\n",
    "            nums.append(int(x[-2]))\n",
    "        if (len(nums) > 0):\n",
    "            n = max(nums)\n",
    "            scope_number = str(n + 1).zfill(3)\n",
    "        else:\n",
    "            scope_number = '001'\n",
    "        print('list of files :', fls)\n",
    "        print('list of numbers :', nums)\n",
    "        print('new scope number = ', scope_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a list of possible embedding models\n",
    "\n",
    "# [m[\"id\"] for m in ls.models.get_embedding_model_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a list of available LLMS for labelling the clusters\n",
    "\n",
    "# [m[\"id\"] for m in ls.models.get_chat_model_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define various parameters to use for latent-scope analysis\n",
    "\n",
    "# response column name from DataFrame\n",
    "text_column = \"student_responses\"\n",
    "\n",
    "# settings for embeddings\n",
    "embedding_model_id = \"transformers-jinaai___jina-embeddings-v2-small-en\"\n",
    "# Ritika says that usually people choose 1000 or 3000 dimensions, but not sure there's a quantitative criteria for that\n",
    "# She heard the rule of thumb: \"use the fourth root of the total number of unique categorical elements while another is that the embedding dimension should be approximately 1.6 times the square root of the number of unique elements in the category, and no less than 600.\"\n",
    "embedding_n_dimensions = 1000\n",
    "\n",
    "# settings for umap\n",
    "# Ritika says 2 components is good, but could try 3 with 10k data points if necessary.  \n",
    "# efren suggests that we cluster on more than 2 dimension...\n",
    "# Ritika suggests keeping min_dist = 0, and then slowly increasing to see if clusters change.\n",
    "# Ritika also suggest to set n_neighbors to 100 (w/ 10k points). 30 is the default.\n",
    "umap_n_components = 3\n",
    "umap_n_neighbors = 30\n",
    "umap_min_dist = 0\n",
    "umap_embedding_id = \"embedding-\" + scope_number\n",
    "\n",
    "# settings for clustering\n",
    "# I should look into the best settings here. \n",
    "# For now I will so I will use the same default values included for the dadabase example that ran through flask \n",
    "cluster_samples = 5\n",
    "cluster_min_samples = 10\n",
    "cluster_selection_epsilon =  0.05\n",
    "cluster_umap_id = \"umap-\" + scope_number\n",
    "\n",
    "# settings for LLM labeller\n",
    "#chat_model_id = \"transformers-HuggingFaceH4___zephyr-7b-beta\"\n",
    "chat_model_id = \"transformers-TinyLlama___TinyLlama-1.1B-Chat-v1.0\"\n",
    "label_length = 10\n",
    "chat_model_instructions_before = \"Below is a list of items each starting with [item].  Each item is a response from a different person to a survey. These items all have a similar theme.  The list begins below.\"\n",
    "chat_model_instructions_after = f\"That was the last item in the list.  Now return a concise label for the items in this list that describes the theme.  This label should not be fully verbatim text from any individual item.  Your label should contain no more than {label_length} words.\"\n",
    "label_cluster_id = \"cluster-\" + scope_number\n",
    "\n",
    "# settings for scope file\n",
    "scope_labels_id = label_cluster_id + \"-labels-\" + scope_number\n",
    "scope_label = \"Scope\" + scope_number\n",
    "scope_description = \"First full test with responses separated into sentences\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run `latent-scope`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingest the data into latent-scope\n",
    "ls.ingest(dataset_id, data, text_column = text_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the embeddings\n",
    "\n",
    "# dataset_id, text_column, model_id, prefix, rerun, dimensions\n",
    "# NOTE: the example notebook online did not have rerun or dimensions.  I looked at the code, and I think rerun should be None\n",
    "#       dimensions from the flask server = 384 (not sure where this number comes from!)\n",
    "\n",
    "ls.embed(dataset_id, text_column, embedding_model_id, \"\", None, embedding_n_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run UMAP dimension reduction\n",
    "\n",
    "# dataset_id, embedding_id, n_neighbors, min_dist\n",
    "# NOTE: I added the n_components arg\n",
    "     \n",
    "ls.umap(dataset_id, umap_embedding_id, umap_n_neighbors, umap_min_dist, n_components = umap_n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run HDBSCAN to cluster (on UMAP vectors)\n",
    "\n",
    "# dataset_id, umap_id, samples, min_samples\n",
    "# NOTE: the example from latent-scope's GitHub repo is missing an argument for \"cluster_selection_epsilon\"... \n",
    "ls.cluster(dataset_id, cluster_umap_id, cluster_samples, cluster_min_samples, cluster_selection_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_id, text_column, cluster_id, model_id, unused, rerun, instructions_before, instructions_after, label_length\n",
    "# NOTE: the code from GitHub was outdated and needed the last arg : rerun = None (or a value that points to a label), I added label_legth\n",
    "ls.label(dataset_id, text_column, label_cluster_id, chat_model_id, \"\", None, chat_model_instructions_before, chat_model_instructions_after,  label_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the labels\n",
    "labels = pd.read_parquet(os.path.join(ls.get_data_dir(), dataset_id, \"clusters\", scope_labels_id + \".parquet\"))\n",
    "labels['label'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save these results as a 'scope'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_id, embedding_id, umap_id, cluster_id, labels_id, label, description\n",
    "ls.scope(dataset_id, umap_embedding_id, cluster_umap_id, label_cluster_id, scope_labels_id, scope_label, scope_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a bar chart showing the number of responses in each theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the labels and read in the original data\n",
    "labels = pd.read_parquet(os.path.join(ls.get_data_dir(), dataset_id, \"clusters\", scope_labels_id + \".parquet\"))\n",
    "data = pd.read_csv(\"../../data/ITP_CourseArtifacts_June_2021_END_of_Course_DeIDENTIFIED_Course_Meta_App_cleaned_split_into_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# match the indices from labels to the original data IDs and count the number of unique entries\n",
    "labels_list = []\n",
    "labels_num = []\n",
    "for index, row in labels.iterrows():\n",
    "    labels_list.append(row['label'])\n",
    "    labels_num.append(len(data.iloc[row['indices']]['ID'].unique()))\n",
    "    print(labels_list[-1], len(row['indices']), labels_num[-1])\n",
    "labels_frac = np.array(labels_num)/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort (by creating a DataFrame)\n",
    "df = pd.DataFrame()\n",
    "df['label'] = labels_list\n",
    "df['frac'] = labels_frac\n",
    "df['num'] = labels_num\n",
    "df.sort_values(by = 'num', inplace = True, ascending = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize = (10,10))\n",
    "\n",
    "y_pos = np.arange(len(df['label']))\n",
    "ax.barh(y_pos, df['frac'], align = 'center')\n",
    "ax.set_yticks(y_pos, labels = df['label'])\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Fraction of responses including the given theme')\n",
    "ax.set_title('Themes from survey responses')\n",
    "\n",
    "f.savefig('plots/' + dataset_id + '.png', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the server to investigate and visualize these results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ls.serve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
