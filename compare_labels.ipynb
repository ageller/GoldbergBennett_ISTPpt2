{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to help compare the labels for all the various options for each question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "froot = \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M0_IF_Reflection_Questions_cleaned\"\n",
    "# froot = \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M1_IF_Reflection_Question_cleaned\"\n",
    "# froot = \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M2_IF_Reflection_Question_cleaned\"\n",
    "\n",
    "embedders = ['UAE', 'bge', 'jina']\n",
    "labellers = ['zephyr','tinyllama']\n",
    "\n",
    "labels_list = []\n",
    "# create an Excel file and add all the sheets\n",
    "with pd.ExcelWriter(os.path.join('tables', froot + '_label_comparison_MC.xlsx'), engine='openpyxl') as writer:\n",
    "    for embedder in embedders:\n",
    "        labels = pd.DataFrame()\n",
    "        for labeller in labellers:\n",
    "            for filename in glob.glob(os.path.join('tables', '*' + froot + '*' + embedder + '*clusters*' + labeller + '*MC.xlsx')):\n",
    "                with open(os.path.join(os.getcwd(), filename), 'r') as f: \n",
    "                    print(filename)\n",
    "                    df = pd.read_excel(filename, sheet_name = 'labels_map')\n",
    "                    labels[labeller] = df['label'].to_list()\n",
    "                    labels_list.extend(df['label'].to_list())\n",
    "                labels.to_excel(writer, sheet_name = embedder, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to use an LLM to combine/summarize all these labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from latent-scope \n",
    "class transformers_chat_provider():\n",
    "    def __init__(self, name, params):\n",
    "        self.name = name\n",
    "        self.params = params\n",
    "        self.pipe = pipeline(\"text-generation\", model=self.name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "        self.encoder = self.pipe.tokenizer\n",
    "\n",
    "    def chat(self, messages, max_new_tokens=24):\n",
    "        prompt = self.pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        outputs = self.pipe(prompt, max_new_tokens=max_new_tokens, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "        generated_text = outputs[0][\"generated_text\"]\n",
    "        print(\"GENERATED TEXT\", generated_text)\n",
    "        return generated_text.split(\"<|assistant|>\")[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions_before = \"Below is a list of items each starting with [item].  Some items may be very similar, while others are different.\"\n",
    "instructions_after = \"That was the last item in the list.  Now summarize these items with a new list of up to 10 themes.  Your themes should describe all the unique ideas from the items above.  Each theme should be only one sentence.  You can return less than 10 themes if there are less than 10 unique ideas.  Only return the themes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# currently this seems to at least be providing output using TinyLlama.  Didn't wait long enough for zephyr.\n",
    "\n",
    "# model = transformers_chat_provider('HuggingFaceH4/zephyr-7b-beta', {\"max_tokens\": 2048})\n",
    "# ofile = os.path.join('tables', froot + '_label_summary_zephyr_MC.xlsx')\n",
    "\n",
    "model = transformers_chat_provider('TinyLlama/TinyLlama-1.1B-Chat-v1.0', {\"max_tokens\": 2048})\n",
    "ofile = os.path.join('tables', froot + '_label_summary_tinyllama_MC.txt')\n",
    "\n",
    "input_list = ''\n",
    "for label in labels_list:\n",
    "    input_list += '[item] ' + label + '\\n'\n",
    "\n",
    "messages=[\n",
    "    {\"role\":\"system\", \"content\":instructions_before},\n",
    "    {\"role\":\"user\", \"content\": input_list},\n",
    "    {\"role\":\"system\", \"content\":instructions_after}\n",
    "]\n",
    "response = model.chat(messages, max_new_tokens = 1000)\n",
    "\n",
    "\n",
    "with open(ofile,'w') as f:\n",
    "    f.write(response)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
