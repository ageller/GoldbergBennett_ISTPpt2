{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the labels for all the various options for each question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# froot = \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M0_IF_Reflection_Questions_cleaned\"\n",
    "# froot = \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M1_IF_Reflection_Question_cleaned\"\n",
    "# froot = \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M2_IF_Reflection_Question_cleaned\"\n",
    "# froot = \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M3_IF_Reflection_Question_cleaned\"\n",
    "# froot = \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M4_IF_Reflection_Question_cleaned\"\n",
    "# froot = \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M5_IF_Reflection_Question_cleaned\"\n",
    "\n",
    "froots = [\n",
    "    \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M0_IF_Reflection_Questions_cleaned\",\n",
    "    \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M1_IF_Reflection_Question_cleaned\",\n",
    "    \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M2_IF_Reflection_Question_cleaned\",\n",
    "    \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M3_IF_Reflection_Question_cleaned\",\n",
    "    \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M4_IF_Reflection_Question_cleaned\",\n",
    "    \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M5_IF_Reflection_Question_cleaned\" ,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Excel file that compares the labels for each embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedders = ['UAE', 'bge', 'jina']\n",
    "labellers = ['zephyr','tinyllama']\n",
    "\n",
    "for froot in froots:\n",
    "    # create an Excel file and add all the sheets\n",
    "    with pd.ExcelWriter(os.path.join('tables', froot + '_label_comparison_MC.xlsx'), engine='openpyxl') as writer:\n",
    "        for embedder in embedders:\n",
    "            labels = pd.DataFrame()\n",
    "            for labeller in labellers:\n",
    "                for filename in glob.glob(os.path.join('tables', '*' + froot + '*' + embedder + '*clusters*' + labeller + '*MC.xlsx')):\n",
    "                    with open(os.path.join(os.getcwd(), filename), 'r') as f: \n",
    "                        print(filename)\n",
    "                        df = pd.read_excel(filename, sheet_name = 'labels_map')\n",
    "                        labels[labeller] = df['label'].to_list()\n",
    "                    labels.to_excel(writer, sheet_name = embedder, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to use an LLM to combine/summarize all these labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from latent-scope \n",
    "class transformers_chat_provider():\n",
    "    def __init__(self, name, params):\n",
    "        self.name = name\n",
    "        self.params = params\n",
    "        self.pipe = pipeline(\"text-generation\", model=self.name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "        self.encoder = self.pipe.tokenizer\n",
    "\n",
    "    def chat(self, messages, max_new_tokens=24):\n",
    "        prompt = self.pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        outputs = self.pipe(prompt, max_new_tokens=max_new_tokens, do_sample=True)#, temperature=0.5, top_p=0.95) #top_k=50, \n",
    "        generated_text = outputs[0][\"generated_text\"]\n",
    "        print(\"GENERATED TEXT\", generated_text)\n",
    "        return generated_text.split(\"<|assistant|>\")[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "froot = froots[5]\n",
    "\n",
    "# embedders = ['UAE', 'bge', 'jina']\n",
    "embedders = ['UAE', 'bge'] #remove jina\n",
    "labellers = ['zephyr','tinyllama']\n",
    "\n",
    "labels_list = []\n",
    "# create an Excel file and add all the sheets\n",
    "for embedder in embedders:\n",
    "    labels = pd.DataFrame()\n",
    "    for labeller in labellers:\n",
    "        for filename in glob.glob(os.path.join('tables', '*' + froot + '*' + embedder + '*clusters*' + labeller + '*MC.xlsx')):\n",
    "            with open(os.path.join(os.getcwd(), filename), 'r') as f: \n",
    "                print(filename)\n",
    "                df = pd.read_excel(filename, sheet_name = 'labels_map')\n",
    "                labels_list.extend(df['label'].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions_before = \"Below is a list of items each starting with [item]. I will want you to summarize this list.\"\n",
    "instructions_after = \"That was the last item in the list.  Now summarize these items with a new list of up to 10 themes.  Your themes should describe all the unique ideas from the items above.  Do not repeat any item from above verbatim in your themes.  Each theme should be only one short sentence.  Only return the short one-sentence themes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model and output file (pick one)\n",
    "\n",
    "model = transformers_chat_provider('HuggingFaceH4/zephyr-7b-beta', {\"max_tokens\": 2048})\n",
    "ofile = os.path.join('tables', froot + '_label_summary_zephyr_MC.txt')\n",
    "\n",
    "# model = transformers_chat_provider('TinyLlama/TinyLlama-1.1B-Chat-v1.0', {\"max_tokens\": 2048})\n",
    "# ofile = os.path.join('tables', froot + '_label_summary_tinyllama_MC.txt')\n",
    "\n",
    "# define the message to the LLM\n",
    "input_list = ''\n",
    "for label in labels_list:\n",
    "    input_list += '[item] ' + label + '\\n'\n",
    "messages=[\n",
    "    {\"role\":\"system\", \"content\":instructions_before},\n",
    "    {\"role\":\"user\", \"content\": input_list},\n",
    "    {\"role\":\"system\", \"content\":instructions_after}\n",
    "]\n",
    "\n",
    "# run the model\n",
    "response = model.chat(messages, max_new_tokens = 1000)\n",
    "\n",
    "# save to file\n",
    "with open(ofile,'w') as f:\n",
    "    f.write(response)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the summary themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "froot = froots[3]\n",
    "\n",
    "print('tinyLlama:')\n",
    "with open(os.path.join('tables', froot + '_label_summary_tinyllama_MC.txt'),'r') as f:\n",
    "    print(f.read())\n",
    "\n",
    "print('')\n",
    "print('zephyr:')\n",
    "with open(os.path.join('tables', froot + '_label_summary_zephyr_MC.txt'),'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Later) Re-label the clusters ?\n",
    "\n",
    "I considered updating the latent-scope code to remove the temperature, top_p, and top_k settings and rerunning all the LLM labels.  I did this for tinyllama, and it didn't really improve things dramatically.  I started for zephyr (which takes MUCH longer), and I also didn't see much improvement.  So I will not do this.  (Though the file below with the cluster parameters is helpful.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a file with the cluster parameters\n",
    "\n",
    "(only need to run this once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = {'fname':[], 'dataset_id':[], 'embedding_model_id':[], 'embedding_n_dimensions':[], 'umap_number':[], 'cluster_number':[]}\n",
    "with open('tables/tmp_best_clusters.txt','r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        x = line.split()\n",
    "        key = x[0].replace('worker.','')\n",
    "        val = x[2].replace(\"'\",'')\n",
    "        out[key].append(val)\n",
    "        if (key == 'dataset_id'):\n",
    "            fname = val.rpartition('_')[0] + '.csv'\n",
    "            out['fname'].append(fname)\n",
    "\n",
    "            embedder = val.rpartition('_')[-1]\n",
    "            if ('UAE' in embedder):\n",
    "                out['embedding_model_id'].append(\"transformers-WhereIsAI___UAE-Large-V1\")\n",
    "                out['embedding_n_dimensions'].append(1024) \n",
    "            if ('bge' in embedder):\n",
    "                out['embedding_model_id'].append(\"transformers-BAAI___bge-large-en-v1.5\")\n",
    "                out['embedding_n_dimensions'].append(1024) \n",
    "            if ('jina' in embedder):\n",
    "                out['embedding_model_id'].append(\"transformers-jinaai___jina-embeddings-v2-small-en\")\n",
    "                out['embedding_n_dimensions'].append(1024) \n",
    "\n",
    "df = pd.DataFrame(out)\n",
    "df['embedding_number'] = '00001'\n",
    "df.to_csv('tables/best_clusters.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through and re-label each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latentscope_helper import latentscope_helper\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker = latentscope_helper(\n",
    "    latent_scope_dir = \"../../latent-scope_data\",\n",
    "    text_column = \"student_responses\",\n",
    "    chat_model_instructions_before = \"Below is a list of items each starting with [item].  Each item is a response from a different person to a survey. These items all have a similar theme.  The list begins below.\", \n",
    "    chat_model_instructions_after = \"That was the last item in the list.  Now return a concise label for the items in this list that describes the theme.  This label should not be fully verbatim text from any individual item.  Your label should contain no more than 10 words.\",\n",
    ")\n",
    "\n",
    "worker.suppress_latentscope_output = worker.suppress_helper_output = worker.remove_old_files = False\n",
    "worker.run_embedding = worker.run_umap = worker.run_cluster = False\n",
    "worker.run_label = worker.save_scope = True\n",
    "\n",
    "# choose one of the following\n",
    "\n",
    "worker.chat_model_id = 'transformers-HuggingFaceH4___zephyr-7b-beta'\n",
    "worker.chat_file_label = 'zephyr'\n",
    "worker.scope_number = '00003'\n",
    "worker.label_number = '00003'\n",
    "\n",
    "# worker.chat_model_id = 'transformers-TinyLlama___TinyLlama-1.1B-Chat-v1.0'\n",
    "# worker.chat_file_label = 'tinyllama'\n",
    "# worker.scope_number = '00004'\n",
    "# worker.label_number = '00004'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tables/best_clusters.csv', dtype=str)\n",
    "for index,row in df.iterrows():\n",
    "    if (index == 2):\n",
    "        print(row['dataset_id'])\n",
    "        \n",
    "        data = pd.read_csv(\"../../data/\" + row['fname'])\n",
    "\n",
    "        worker.data = data\n",
    "        worker.dataset_id = row['dataset_id']\n",
    "        worker.embedding_number = row['embedding_number']\n",
    "        worker.umap_number = row['umap_number']\n",
    "        worker.cluster_number = row['cluster_number']\n",
    "\n",
    "        worker.initialize_files_and_numbering()\n",
    "        worker.initialize_latentscope()\n",
    "        worker.run_latentscope()\n",
    "\n",
    "        f, ax = worker.create_bar_chart(filename = os.path.join('plots', worker.dataset_id + '_' + worker.chat_file_label + '_bar_MC.png'))\n",
    "\n",
    "        data_raw = data.copy()\n",
    "        worker.create_excel_workbook(data_raw, os.path.join('tables', worker.dataset_id + '_clusters_' + worker.chat_file_label + '_MC.xlsx'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking difference between default and updated parameters\n",
    "\n",
    "(For the ones I checked, they don't look different enough to justify this change or spending time running zephyr for all.  Aborting.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = 'UAE'\n",
    "labeller = 'tinyllama' #zephyr\n",
    "froot = froots[5]\n",
    "\n",
    "# again, I don't need this as a for loop since it should only pick up one file\n",
    "for filename in glob.glob(os.path.join('tables', '*' + froot + '*' + embedder + '*clusters*' + labeller + '*MC.xlsx')):\n",
    "    with open(os.path.join(os.getcwd(), filename), 'r') as f: \n",
    "        print(filename)\n",
    "        df = pd.read_excel(filename, sheet_name = 'labels_map')\n",
    "        labels_new = df['label'].to_list()\n",
    "\n",
    "for filename in glob.glob(os.path.join('tables', 'latentscope_defaults','*' + froot + '*' + embedder + '*clusters*' + labeller + '*MC.xlsx')):\n",
    "    with open(os.path.join(os.getcwd(), filename), 'r') as f: \n",
    "        print(filename)\n",
    "        df = pd.read_excel(filename, sheet_name = 'labels_map')\n",
    "        labels_default = df['label'].to_list()\n",
    "\n",
    "for x,y in zip(labels_new, labels_default):\n",
    "    print()\n",
    "    print('new:',x)\n",
    "    print('old:',y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
