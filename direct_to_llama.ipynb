{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try sending answers directly to the Llama LLM (local)\n",
    "\n",
    "Using Llama 4 which has a context window of 10M tokens (!)  Specifically, I'll use `meta-llama/Llama-4-Scout-17B-16E-Instruct` from : https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct\n",
    "\n",
    "Create a conda env , but install via pip because pypi typically has the more up to date versions for huggingface.\n",
    "```\n",
    "conda create -n llama4 python=3.10\n",
    "conda activate llama4\n",
    "pip install torch transformers accelerate pandas jupyter\n",
    "```\n",
    "\n",
    "... but this would require too much memory for me to run on my local computer (apparently I would need at leat 80GB of VRAM -- so this would need to be on Quest).  Also at this point I would need to request access on Hugging face (and wait for approval).  So I haven't run this code yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoProcessor, Llama4ForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "froots = [\n",
    "    \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M0_IF_Reflection_Questions_cleaned\",\n",
    "    \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M1_IF_Reflection_Question_cleaned\",\n",
    "    \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M2_IF_Reflection_Question_cleaned\",\n",
    "    \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M3_IF_Reflection_Question_cleaned\",\n",
    "    \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M4_IF_Reflection_Question_cleaned\",\n",
    "    \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M5_IF_Reflection_Question_cleaned\" ,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of words in each file\n",
    "for f in froots:\n",
    "    df = pd.read_csv(\"../../data/\" + f + \".csv\")\n",
    "    all_answers = df['student_responses'].str.cat(sep=' ').split(' ')\n",
    "    print(f, len(all_answers)*4/3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model (code from Hugging Face : https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct)\n",
    "# Note: \"instruct\" models are meant to follow user instructions more closely (so we want to use that)\n",
    "model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = Llama4ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    attn_implementation=\"flex_attention\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instructions for the prompt to bracket the survey responses\n",
    "instructions_before = \"Below is a list of items each starting with [item]. I will want you to summarize this list.\"\n",
    "instructions_after = \"That was the last item in the list.  Now summarize these items with a new list of 10 or less unique themes.  Do not repeat any item from above verbatim in your themes.  Each theme should be only one short sentence.  Only return the short one-sentence themes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the full prompt and run it through the model\n",
    "\n",
    "# could write a for loop to go through all files (and create output files), but let's first just try one\n",
    "i = 0\n",
    "froot = froots[0]\n",
    "\n",
    "input_list = ''\n",
    "df = pd.read_csv(\"../../data/\" + froot + \".csv\")\n",
    "responses = df['student_responses'].to_list()\n",
    "for item in responses:\n",
    "    input_list += '[item] ' + item + '\\n'\n",
    "prompt = instructions_before + '\\n' + input_list + '\\n' + instructions_after\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "# Tokenize input\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "# Decode output\n",
    "response = processor.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])[0]\n",
    "print(response)\n",
    "print(outputs[0])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama4-wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
