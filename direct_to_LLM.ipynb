{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try sending answers in batches directly to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "froots = [\n",
    "    \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M0_IF_Reflection_Questions_cleaned\",\n",
    "    \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M1_IF_Reflection_Question_cleaned\",\n",
    "    \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M2_IF_Reflection_Question_cleaned\",\n",
    "    \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M3_IF_Reflection_Question_cleaned\",\n",
    "    \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M4_IF_Reflection_Question_cleaned\",\n",
    "    \"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M5_IF_Reflection_Question_cleaned\" ,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from latent-scope \n",
    "class transformers_chat_provider():\n",
    "    def __init__(self, name, params):\n",
    "        self.name = name\n",
    "        self.params = params\n",
    "        self.pipe = pipeline(\"text-generation\", model=self.name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "        self.encoder = self.pipe.tokenizer\n",
    "\n",
    "    def chat(self, messages, max_new_tokens=24):\n",
    "        prompt = self.pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        outputs = self.pipe(prompt, max_new_tokens=max_new_tokens, do_sample=True)#, temperature=0.5, top_p=0.95) #top_k=50, \n",
    "        generated_text = outputs[0][\"generated_text\"]\n",
    "        print(\"GENERATED TEXT\", generated_text)\n",
    "        return generated_text.split(\"<|assistant|>\")[1].strip()\n",
    "    \n",
    "def get_LLM_input_max_index(model, input_list, max_tokens = 1048, start_index = 0, system_text = '', extra_tokens_per_item = 6):\n",
    "    # determine how many lines I can feed into the LLM\n",
    "\n",
    "    n_tokens = len(model.encoder.encode(system_text))    \n",
    "    for index, item in enumerate(input_list):\n",
    "        if (index > start_index):\n",
    "            encoded_text = model.encoder.encode(item)\n",
    "            n_tokens += len(encoded_text) + extra_tokens_per_item\n",
    "            if (n_tokens > max_tokens):\n",
    "                return index - 1\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions_before = \"Below is a list of items each starting with [item]. I will want you to summarize this list.\"\n",
    "instructions_after = \"That was the last item in the list.  Now summarize these items with a new list of 10 or less unique themes.  Do not repeat any item from above verbatim in your themes.  Each theme should be only one short sentence.  Only return the short one-sentence themes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "froot = froots[0]\n",
    "df = pd.read_csv(\"../../data/\" + froot + \".csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 2048\n",
    "max_new_tokens = 500\n",
    "\n",
    "model = transformers_chat_provider('TinyLlama/TinyLlama-1.1B-Chat-v1.0', {\"max_tokens\": max_tokens})\n",
    "ofile = os.path.join('tables', froot + '_themes_directly_from_tinyllama.txt')\n",
    "\n",
    "# model = transformers_chat_provider('HuggingFaceH4/zephyr-7b-beta', {\"max_tokens\": max_tokens})\n",
    "# ofile = os.path.join('tables', froot + '_label_summary_zephyr_MC.txt')\n",
    "\n",
    "responses = df['student_responses'].to_list()\n",
    "start_index = 0\n",
    "end_index = get_LLM_input_max_index(model, responses, start_index = start_index, max_tokens = max_tokens - max_new_tokens)\n",
    "print(end_index)\n",
    "\n",
    "# define the message to the LLM\n",
    "input_list = ''\n",
    "for item in responses[start_index:(end_index - start_index)]:\n",
    "    input_list += '[item] ' + item + '\\n'\n",
    "messages=[\n",
    "    {\"role\":\"system\", \"content\":instructions_before},\n",
    "    {\"role\":\"user\", \"content\": input_list},\n",
    "    {\"role\":\"system\", \"content\":instructions_after}\n",
    "]\n",
    "\n",
    "# run the model\n",
    "response = model.chat(messages, max_new_tokens = 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of words in each file\n",
    "# from OpenAI: https://platform.openai.com/tokenizer \"A helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughly Â¾ of a word (so 100 tokens ~= 75 words).\"\n",
    "# the largest GPT model has 128K tokens context limit (so it won't be able to ingest everything)\n",
    "for f in froots:\n",
    "    df = pd.read_csv(\"../../data/\" + f + \".csv\")\n",
    "    all_answers = df['student_responses'].str.cat(sep=' ').split(' ')\n",
    "    print(f, len(all_answers)*4/3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
