{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Bennett's Data\n",
    "\n",
    "`ITP_CourseArtifacts_June 2021_END_of_Course_DeIDENTIFIED.xlsx` using the `Course Meta SelfEff` sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from latentscope_helper import latentscope_helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in and clean the data\n",
    "\n",
    "These next two cells only need to be run once.  (If rerunning this notebook, you can start after the next markdown cell.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this to True if you want to change the data the is used \n",
    "read_in_original_data_file = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into sentences\n",
    "def split_into_sentences(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (read_in_original_data_file):\n",
    "    # read in the data\n",
    "    df = pd.read_excel(\"../../data/ITP_CourseArtifacts_June 2021_END_of_Course_DeIDENTIFIED.xlsx\", sheet_name = \"Course Meta SelfEff\")\n",
    "    print(f\"length of original DataFrame = {len(df)}\")\n",
    "\n",
    "    # save the question\n",
    "    question = df.columns[1]\n",
    "    # only take the columns we need and rename them to remove spaces and special characters\n",
    "    data_tmp = df.rename(columns = {question:'student_responses'})\n",
    "\n",
    "    # remove extra newlines\n",
    "    data_tmp['student_responses'] = data_tmp['student_responses'].str.replace('\\n', ' ')\n",
    "    \n",
    "    # remove rows with short answers (otherwise the sentence finder might choke -- not sure why)\n",
    "    n_min = 5\n",
    "    data_tmp = data_tmp[data_tmp['student_responses'].str.split().str.len().gt(n_min)]  \n",
    "\n",
    "    # save to .csv file\n",
    "    data_tmp.to_csv(\"../../data/ITP_CourseArtifacts_June_2021_END_of_Course_DeIDENTIFIED_Course_Meta_SelfEff_cleaned.csv\", index=False)\n",
    "\n",
    "    # split into sentences\n",
    "    data = pd.DataFrame(columns=data_tmp.columns)\n",
    "    for index, row in data_tmp.iterrows():\n",
    "        # Split the response into sentences\n",
    "        sentences = split_into_sentences(row['student_responses'])\n",
    "        \n",
    "        # Create a new row for each sentence and append it to the new DataFrame\n",
    "        for sentence in sentences:\n",
    "            new_row = row.copy()\n",
    "            new_row['student_responses'] = sentence\n",
    "            data = data._append(new_row, ignore_index=True)\n",
    "\n",
    "    # remove rows with short answers (again)\n",
    "    n_min = 5\n",
    "    data = data[data['student_responses'].str.split().str.len().gt(n_min)]  \n",
    "\n",
    "    print(f\"length of new DataFrame (after cleaning and sentence splitting) = {len(data)}\")\n",
    "\n",
    "    # Save the new DataFrame to a new file (since this takes a while to run)\n",
    "    data.to_csv(\"../../data/ITP_CourseArtifacts_June_2021_END_of_Course_DeIDENTIFIED_Course_Meta_SelfEff_cleaned_split_into_sentences.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and run `latent-scope` using my Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This question has multiple components; so definitely best to take the version with split sentences\n",
    "data = pd.read_csv(\"../../data/ITP_CourseArtifacts_June_2021_END_of_Course_DeIDENTIFIED_Course_Meta_SelfEff_cleaned_split_into_sentences.csv\")\n",
    "\n",
    "# initialize my helper object\n",
    "worker = latentscope_helper(\n",
    "    latent_scope_dir = \"../../latent-scope_data\", # directory where the latentscope files are stored\n",
    "    dataset_id = \"ITP_CourseArtifacts_June_2021_END_of_Course_DeIDENTIFIED_Course_Meta_SelfEff\", # data set name, for the sub-directory name within latent_scope_dir for this project\n",
    "    data = data, # pandas DataFrame that contains the data to analyze\n",
    "    text_column = \"student_responses\", # response column name from data_file\n",
    "    remove_old_files = True, # set this to True if you want to clean the latent-scope directories and start fresh\n",
    "    quick_clean = True, # set this to True if you want to remove every file in the directories, regardless of imin and imax\n",
    "    imin = 0, # minimum number of for files to search through (to remove)\n",
    "    imax = 50, # maximum number of for files to search through (to remove)\n",
    "    run_embedding = True, # whether to run the embedding step (and potentially remove previous files)\n",
    "    run_umap = True, # whether to run the umap step (and potentially remove previous files)\n",
    "    run_cluster = True, # whether to run the clustering step (and potentially remove previous files)\n",
    "    run_label = True, # whether to run the labeling step (and potentially remove previous files)\n",
    "    embedding_model_id = \"transformers-jinaai___jina-embeddings-v2-small-en\", # embeddings model name\n",
    "    embedding_n_dimensions = 512, # number of dimensions for embedding.  reading the jina docs, they often use this number as an example for the number of dimensions (not sure this is a recommendation though)\n",
    "    umap_n_components = 2, # number of UMAP dimensions\n",
    "    umap_n_neighbors = 10, # \"controls how UMAP balances local versus global structure in the data.\" Larger values mean UMAP will look at larger distances for neighbors (15 is default)\n",
    "    umap_min_dist = 0, # \"controls how tightly UMAP is allowed to pack points together\" (default is 0.1)\n",
    "    cluster_samples = 5, # min_cluster_size in HDBSCAN : \"the smallest size grouping that you wish to consider a cluster\"\n",
    "    cluster_min_samples = 12, # min_samples in HDBSCAN : \"provide a measure of how conservative you want your clustering to be. The larger the value of min_samples you provide, the more conservative the clustering â€“ more points will be declared as noise, and clusters will be restricted to progressively more dense areas.\"\n",
    "    cluster_selection_epsilon =  0.05, # cluster_selection_epsilon in HDBSCAN : distance measure between clusters that \"ensures that clusters below the given threshold are not split up any further\"\n",
    "    chat_model_id = \"transformers-TinyLlama___TinyLlama-1.1B-Chat-v1.0\", # LLM to use for labeling the clusters\n",
    "    label_length = 10, # max length to tell the LLM to use when generating a given label (not always respected by the LLM!)\n",
    "    chat_model_instructions_before = \"Below is a list of items each starting with [item].  Each item is a response from a different person to a survey. These items all have a similar theme.  The list begins below.\", # string of text to provide the LLM as instructions before the list of cluster items is given\n",
    "    chat_model_instructions_after = \"That was the last item in the list.  Now return a concise label for the items in this list that describes the theme.  This label should not be fully verbatim text from any individual item.  Your label should contain no more than 10 words.\", # string of text to provide the LLM as instructions after the list of cluster items is given\n",
    "    scope_description = \"First full test with responses separated into sentences\", #label to give the scope (when using the latentscope server to view the data)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run latent-scope (using the inputs from above)\n",
    "\n",
    "# if embeddings already exist, no need to run them again (unless indending to change embedding parameters)\n",
    "# worker.run_embedding = False\n",
    "\n",
    "worker.save_scope = False # I will not save this one, but I will save the scope below after the metrics\n",
    "\n",
    "worker.initialize_files_and_numbering()\n",
    "worker.initialize_latentscope()\n",
    "worker.run_latentscope()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metrics to assess the quality of this analysis\n",
    "\n",
    "Ideally, I would want to do this for a number of runs each changing some parameter and returning a different number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# returns inertia, Silhouette Coefficient, Calinski-Harabasz Index, Davies-Bouldin Index\n",
    "# - a lower inertia value is generally better\n",
    "# - a higher Silhouette Coefficient score relates to a model with better defined clusters. \n",
    "# - a higher Calinski-Harabasz score relates to a model with better defined clusters.\n",
    "# - a lower Davies-Bouldin index relates to a model with better separation between the clusters.\n",
    "worker.calculate_metrics(embedding_number = '00001', umap_number = '00001', cluster_number = '00001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove some files?\n",
    "# worker.remove_old_files = True\n",
    "# worker.suppress_helper_output = False\n",
    "# worker.imax = 999\n",
    "# worker.imin = 2\n",
    "# worker.initialize_files_and_numbering()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "# %%capture magic to suppress output\n",
    "# TO DO: Figure out how to supress the figures!\n",
    "\n",
    "\n",
    "# a test looping over one HDBSCAN parameter to check the resulting metrics\n",
    "worker.suppress_latentscope_output = True\n",
    "worker.suppress_helper_output = True\n",
    "worker.remove_old_files = False\n",
    "worker.initialize_files_and_numbering()\n",
    "\n",
    "worker.embedding_number = '00001'\n",
    "worker.run_embedding = worker.run_umap = worker.run_label = worker.run_cluster = worker.save_scope = False\n",
    "\n",
    "umap_n_components = [2, 3]\n",
    "umap_n_neighbors = [5, 10, 15, 20, 25, 30]\n",
    "umap_min_dist = [0, 0.05, 0.1, 0.15]\n",
    "\n",
    "cluster_min_samples = [5, 10, 15, 20, 25, 30]\n",
    "cluster_selection_epsilon = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]\n",
    "\n",
    "un = 2 # starting number for umap files\n",
    "cn = 2 # starting number for the cluster files\n",
    "\n",
    "cluster_file_numbers = []\n",
    "umap_file_numbers = []\n",
    "\n",
    "for u_nc in umap_n_components:\n",
    "    for u_nn in umap_n_neighbors:\n",
    "        for u_md in umap_min_dist:\n",
    "            print(f'umap_file_number = {un}, n_components = {u_nc}, n_neighbors = {u_nn}, min_dist = {u_md}')\n",
    "            worker.umap_n_components = u_nc\n",
    "            worker.umap_n_neighbors = u_nn\n",
    "            worker.umap_min_dist = u_md\n",
    "            worker.umap_number = str(un).zfill(5)\n",
    "            worker.run_umap = True\n",
    "            worker.run_cluster = False\n",
    "            worker.initialize_latentscope_filenames()\n",
    "            worker.run_latentscope()\n",
    "\n",
    "            \n",
    "            # cluster parameters\n",
    "            for c_ms in cluster_min_samples:\n",
    "                for c_eps in cluster_selection_epsilon:\n",
    "                    print(f'cluster_file_number = {cn}, min_samples = {c_ms}, selection_epsilon = {c_eps}')\n",
    "                    worker.cluster_min_samples = c_ms\n",
    "                    worker.cluster_selection_epsilon = c_eps\n",
    "                    worker.cluster_number = str(cn).zfill(5)\n",
    "                    worker.run_umap = False\n",
    "                    worker.run_cluster = True\n",
    "                    worker.initialize_latentscope_filenames()\n",
    "                    worker.run_latentscope()\n",
    "        \n",
    "                    cluster_file_numbers.append(cn)\n",
    "                    umap_file_numbers.append(un)\n",
    "                    cn += 1\n",
    "                    \n",
    "            un += 1\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in case I need to recreate the file numbering structure\n",
    "# for u_nc in umap_n_components:\n",
    "#     for u_nn in umap_n_neighbors:\n",
    "#         for u_md in umap_min_dist:\n",
    "#             print(f'umap_file_number = {un}, n_components = {u_nc}, n_neighbors = {u_nn}, min_dist = {u_md}')\n",
    "            \n",
    "#             # cluster parameters\n",
    "#             for c_ms in cluster_min_samples:\n",
    "#                 for c_eps in cluster_selection_epsilon:\n",
    "#                     print(f'cluster_file_number = {cn}, min_samples = {c_ms}, selection_epsilon = {c_eps}')\n",
    "        \n",
    "#                     cluster_file_numbers.append(cn)\n",
    "#                     umap_file_numbers.append(un)\n",
    "#                     cn += 1\n",
    "                    \n",
    "#             un += 1\n",
    "#             print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "inertia = []\n",
    "sc = []\n",
    "ch = []\n",
    "db = []\n",
    "n_neighbors = []\n",
    "min_dist = []\n",
    "n_components = []\n",
    "min_samples = []\n",
    "selection_epsilon = []\n",
    "n_clusters = []\n",
    "for (un,cn) in zip(umap_file_numbers, cluster_file_numbers):\n",
    "    print(un, cn)\n",
    "    m = worker.calculate_metrics(embedding_number = '00001', umap_number = str(un).zfill(5), cluster_number = str(cn).zfill(5))\n",
    "    metrics.append(m)\n",
    "    inertia.append(m['inertia'])\n",
    "    sc.append(m['silhouette_coefficient'])\n",
    "    ch.append(m['calinski_harabasz_index'])\n",
    "    db.append(m['davies_bouldin_index'])\n",
    "    n_components.append(m['umap_info']['n_components'])\n",
    "    n_neighbors.append(m['umap_info']['neighbors'])\n",
    "    min_dist.append(m['umap_info']['min_dist'])\n",
    "    min_samples.append(m['cluster_info']['min_samples'])\n",
    "    selection_epsilon.append(m['cluster_info']['cluster_selection_epsilon'])\n",
    "    n_clusters.append(m['cluster_info']['n_clusters'])\n",
    "df = pd.DataFrame()\n",
    "df['umap_file_number'] = umap_file_numbers\n",
    "df['cluster_file_number'] = cluster_file_numbers\n",
    "df['umap_n_components'] = n_components\n",
    "df['umap_n_neighbors'] = n_neighbors\n",
    "df['umap_min_dist'] = min_dist\n",
    "df['cluster_min_samples'] = min_samples\n",
    "df['cluster_selection_epsilon'] = selection_epsilon\n",
    "df['n_clusters'] = n_clusters\n",
    "df['inertia'] = inertia\n",
    "df['silhouette_coefficient'] = sc\n",
    "df['calinski_harabasz_index'] = ch\n",
    "df['davies_bouldin_index'] = db\n",
    "\n",
    "df.to_csv(os.path.join('tables', worker.dataset_id + '_metrics_grid.csv'), index = False)\n",
    "\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm still having trouble resetting the plotting backend given the suppression above\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = pd.read_csv(os.path.join('tables', worker.dataset_id + '_metrics_grid.csv'))\n",
    "dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_plot = dfm.columns[2:].tolist()\n",
    "g = sns.pairplot(dfm[cols_to_plot], corner = True, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vars = ['umap_n_components','umap_n_neighbors','umap_min_dist','cluster_min_samples','cluster_selection_epsilon','n_clusters']\n",
    "y_vars = ['silhouette_coefficient','calinski_harabasz_index','davies_bouldin_index', 'inertia']\n",
    "g = sns.PairGrid(dfm, x_vars = x_vars, y_vars = y_vars, hue = 'n_clusters', palette = 'viridis')\n",
    "g.map(sns.scatterplot)\n",
    "g.map_diag(sns.histplot)\n",
    "\n",
    "# I played around with the loc statement to isolate what appears to be the best clusters\n",
    "# (and asked ChatGPT how to create this overlay)\n",
    "dfm_best = df.loc[(dfm['n_clusters'] > 5) & (df['n_clusters'] < 30) & (df['silhouette_coefficient'] > 0.4) & (df['calinski_harabasz_index'] > 2000) & (df['davies_bouldin_index'] <2)]\n",
    "def custom_scatter(x, y, **kwargs):\n",
    "    sns.scatterplot(x=x, y=y, **kwargs)\n",
    "    sns.scatterplot(data=dfm_best, x=x.name, y=y.name, marker='o', color='black', edgecolor='black', facecolors='none', s=100)\n",
    "g.map(custom_scatter)\n",
    "\n",
    "g.savefig(os.path.join('plots', worker.dataset_id + '_metric_grid.png'), bbox_inches = 'tight') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick one of these to create labels and save the scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "worker.suppress_latentscope_output = False\n",
    "worker.suppress_helper_output = False\n",
    "worker.remove_old_files = False\n",
    "worker.embedding_number = '00001'\n",
    "worker.umap_number = '00006'\n",
    "worker.cluster_number = '00276'\n",
    "worker.scope_number = '00001'\n",
    "\n",
    "worker.run_embedding = worker.run_umap = worker.run_label = worker.run_cluster = False\n",
    "worker.run_label = worker.save_scope = True\n",
    "\n",
    "worker.initialize_files_and_numbering()\n",
    "worker.initialize_latentscope()\n",
    "worker.run_latentscope()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print and save the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to plot a specific scope number, you can define it here (and you don't need to actually run latentscope in the previous cell)\n",
    "# worker.remove_old_files = False\n",
    "# worker.scope_number = '00001'\n",
    "# worker.initialize_files_and_numbering()\n",
    "# worker.initialize_latentscope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the labels\n",
    "# self.scope_labels_id = '00001'\n",
    "worker.print_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bar chart of the labels\n",
    "f, ax = worker.create_bar_chart(filename = os.path.join('plots', worker.dataset_id + '_scope' + worker.scope_number + '.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an Excel workbook to review the results\n",
    "# The first sheet will have the raw data.  \n",
    "# The second sheet will have a map between cluster label and sheet name.  \n",
    "# Subsequent sheets will be one per cluster containing the cluster data.\n",
    "data_raw = pd.read_excel(\"../../data/ITP_CourseArtifacts_June 2021_END_of_Course_DeIDENTIFIED.xlsx\", sheet_name = \"Course Meta SelfEff\")\n",
    "worker.create_excel_workbook(data_raw, os.path.join('tables', worker.dataset_id + '_clusters_scope' + worker.scope_number + '.xlsx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the server to investigate and visualize these results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ls.serve()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
