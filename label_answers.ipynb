{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempting to label each answer by the correct theme\n",
    "\n",
    "I will use an embedding model to calculate the vectors for each answer and each theme in a given question, then calculate the distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!! I NEED TO CHECK WITH BENNETT ABOUT HOW HE WANTS TO LABEL THEMES !!!\n",
    "(should we allow for multiple themes for a given answer?  How do we want to determine the threshold for a theme to be associated with an answer?  Currently I take anything greater than the median of all max similarity measurements, or the max value if all are <> the median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will download the model if needed\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "froots = {\n",
    "    \"M0\":\"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M0_IF_Reflection_Questions_cleaned\",\n",
    "    \"M1\":\"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M1_IF_Reflection_Question_cleaned\",\n",
    "    \"M2\":\"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M2_IF_Reflection_Question_cleaned\",\n",
    "    \"M3\":\"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M3_IF_Reflection_Question_cleaned\",\n",
    "    \"M4\":\"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M4_IF_Reflection_Question_cleaned\",\n",
    "    \"M5\":\"INCLU1x_IF_Responses_-_ALL_RUNS_041924_M5_IF_Reflection_Question_cleaned\" ,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qname = 'M0'\n",
    "\n",
    "answers_df = pd.read_csv(\"../../data/\" + froots[qname] + \".csv\")\n",
    "themes_df = pd.read_csv(\"tables/openAI/Aaron_ChatGPT_summary/final_merged/\" + qname + \"_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = answers_df['student_responses'].to_list()\n",
    "themes = themes_df['theme'].to_list()\n",
    "\n",
    "\n",
    "answers_embeddings = model.encode(answers)\n",
    "themes_embeddings = model.encode(themes)\n",
    "\n",
    "similarity = np.array(util.cos_sim(answers_embeddings, themes_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some diagnostics\n",
    "\n",
    "print(similarity.shape, len(answers), len(themes))\n",
    "\n",
    "# Get the maximum value in each row\n",
    "max_values = np.max(similarity, axis=1)\n",
    "\n",
    "# Get the column indices of the maximum values in each row\n",
    "max_indices = np.argmax(similarity, axis=1)\n",
    "\n",
    "s_lo, s_med, s_hi = np.percentile(similarity.ravel(), [16, 50, 84])\n",
    "print(s_lo, s_med, s_hi)\n",
    "\n",
    "smax_lo, smax_med, smax_hi = np.percentile(max_values, [16, 50, 84])\n",
    "print(smax_lo, smax_med, smax_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "_ = ax.hist(similarity.ravel(), bins = 100)\n",
    "ax.axvline(s_med, color = 'black')\n",
    "ax.axvline(s_lo, color = 'black', linestyle = 'dashed')\n",
    "ax.axvline(s_hi, color = 'black', linestyle = 'dashed')\n",
    "ax.set_xlim(0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "_ = ax.hist(max_values, bins = 100)\n",
    "ax.axvline(smax_med, color = 'black')\n",
    "ax.axvline(smax_lo, color = 'black', linestyle = 'dashed')\n",
    "ax.axvline(smax_hi, color = 'black', linestyle = 'dashed')\n",
    "ax.set_xlim(0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the array\n",
    "df = pd.DataFrame(similarity, columns=themes_df['theme_number'])\n",
    "\n",
    "# take any column that has a value greater than the median for the max values OR the column at the max value if all distances are < smax_med\n",
    "def find_columns(row, threshold):\n",
    "    # Find the indices where values are less than the threshold\n",
    "    indices = row.index[row > threshold].tolist()\n",
    "    if not indices:  # If no values are less than the threshold\n",
    "        indices = [row.idxmax()]  # Take the index of the maximum value\n",
    "    return str(indices)\n",
    "\n",
    "output_df = pd.DataFrame()\n",
    "output_df['theme'] = df.apply(lambda row: find_columns(row, smax_med), axis=1)\n",
    "\n",
    "# add the rest of the values from the original df\n",
    "for c in answers_df.columns[::-1]:\n",
    "    output_df.insert(0,c, answers_df[c])\n",
    "\n",
    "output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull everything together into functions so that I can run through all answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data\n",
    "def read_answers_and_themes(qname):\n",
    "    # read in the data for this question\n",
    "    answers_df = pd.read_csv(\"../../data/\" + froots[qname] + \".csv\")\n",
    "    themes_df = pd.read_csv(\"tables/openAI/Aaron_ChatGPT_summary/final_merged/\" + qname + \"_merged.csv\")\n",
    "\n",
    "    # grab t6he answers and themes for this question as lists\n",
    "    answers = answers_df['student_responses'].to_list()\n",
    "    themes = themes_df['theme'].to_list()\n",
    "\n",
    "    return answers_df, answers, themes_df, themes\n",
    "\n",
    "def read_themes_and_themes(qname):\n",
    "    # read in the list of themes for this question\n",
    "    themes_df = pd.read_csv(\"tables/openAI/Aaron_ChatGPT_summary/compiled_themes_csvs/\" + qname + \"_themes_compiled.csv\")\n",
    "    # get the themes, but remove some common phrases\n",
    "    themes = themes_df['theme'].str.lower().replace('inclusive teaching','').str.replace('inclusive practices','').str.replace('inclusive','')\n",
    "    themes = themes.to_list()\n",
    "\n",
    "    return themes_df, themes, themes_df, themes\n",
    "\n",
    "\n",
    "# get all the matching themes for a given set of answers\n",
    "def get_themes(qname, read_func = read_answers_and_themes):\n",
    "    \n",
    "    answers_df, answers, themes_df, themes = read_func(qname)\n",
    "\n",
    "    # calculate embeddings\n",
    "    answers_embeddings = model.encode(answers)\n",
    "    themes_embeddings = model.encode(themes)\n",
    "\n",
    "    # calculate the similarity matrix\n",
    "    similarity = np.array(util.cos_sim(answers_embeddings, themes_embeddings))\n",
    "\n",
    "    # Get the maximum value in each row and then get the median value for a threshold (could modify this)\n",
    "    max_values = np.max(similarity, axis=1)\n",
    "    smax_lo, smax_med, smax_hi = np.percentile(max_values, [16, 50, 84])\n",
    "\n",
    "    # Create a DataFrame from the similarity array\n",
    "    df = pd.DataFrame(similarity, columns=themes_df['theme_number'])\n",
    "\n",
    "    ####################\n",
    "    # We may want to modify this, e.g., to only take the max value (1 theme per answer)\n",
    "    ####################\n",
    "    # take any column that has a value greater than the median for the max values OR the column at the max value if all distances are < smax_med\n",
    "    def find_columns(row, threshold):\n",
    "        # Find the indices where values are less than the threshold\n",
    "        indices = row.index[row > threshold].tolist()\n",
    "        if not indices:  # If no values are less than the threshold\n",
    "            indices = [row.idxmax()]  # Take the index of the maximum value\n",
    "        return str(indices)\n",
    "\n",
    "    output_df = pd.DataFrame()\n",
    "    output_df['theme'] = df.apply(lambda row: find_columns(row, smax_med), axis=1)\n",
    "\n",
    "    # add the rest of the values from the original df\n",
    "    for c in answers_df.columns[::-1]:\n",
    "        cinsert = c\n",
    "        if (c in output_df.columns):\n",
    "            cinsert = c + '_org'\n",
    "        output_df.insert(0,cinsert, answers_df[c])\n",
    "\n",
    "    return output_df, themes_df, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bar chart (similar to my script from latentscope_helper.py)\n",
    "def create_bar_chart(themes_df, output_df, filename = None):\n",
    "\n",
    "    # match the indices from labels to the original data IDs and count the number of unique entries\n",
    "    \n",
    "    # first get a list of all the matching themes\n",
    "    flattened_themes = [int(item) for sublist in output_df['theme'].str.strip(\"[]\").str.split(\", \") for item in sublist]\n",
    "    \n",
    "    # Count the occurrences of each unique value\n",
    "    value_counts = pd.Series(flattened_themes).value_counts().sort_index()\n",
    "    \n",
    "    # Create a new DataFrame from the counts\n",
    "    df_counts = pd.DataFrame({\n",
    "        'theme_number': value_counts.index,\n",
    "        'count': value_counts.values\n",
    "    })\n",
    "    df_counts['frac'] = df_counts['count']/len(output_df.index)\n",
    "\n",
    "    # merge that with the themes_df\n",
    "    df = pd.merge(df_counts, themes_df, on='theme_number', how='left')\n",
    "    \n",
    "    # sort \n",
    "    df.sort_values(by='frac', inplace=True, ascending=False)\n",
    "\n",
    "    # create the figure and save it\n",
    "    f, ax = plt.subplots(figsize = (10,10))\n",
    "    y_pos = np.arange(len(df['theme']))\n",
    "    hbars = ax.barh(y_pos, df['frac'], align = 'center')\n",
    "    ax.bar_label(hbars, labels=[f'{v*100:.0f}%' for v in df['frac']], fontsize=14)\n",
    "    ax.set_yticks(y_pos, labels = df['theme'])\n",
    "    ax.invert_yaxis()  # labels read top-to-bottom\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.xaxis.set_ticks([])\n",
    "\n",
    "    if (filename is not None):\n",
    "        f.savefig(filename, bbox_inches = 'tight')\n",
    "\n",
    "    return f, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process a given question\n",
    "output_df, themes_df, similarity_df = get_themes(\"M0\")\n",
    "_ = create_bar_chart(themes_df, output_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See if I can use embeddings to identify duplicates in the original themes list\n",
    "\n",
    "I don't know if this is better or worse than simply asking ChatGPT :)  Probably the best thing would be for Bennett to go through the list of themes himself and get to a final unique set (without code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes_df, themes, themes_df, themes = read_themes_and_themes(\"M0\")\n",
    "themes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df, themes_df, similarity_df = get_themes(\"M0\", read_func = read_themes_and_themes)\n",
    "similarity_df.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask for the upper triangle (excluding the diagonal)\n",
    "mask = np.triu(np.ones(similarity_df.shape), k=1).astype(bool)\n",
    "\n",
    "# Apply the mask to get the upper triangle values\n",
    "upper_triangle_values = similarity_df.where(mask).stack()\n",
    "\n",
    "# Plotting the histogram\n",
    "_ = plt.hist(upper_triangle_values, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dups(qname, threshold = 0.75):\n",
    "    output_df, themes_df, similarity_df = get_themes(qname, read_func = read_themes_and_themes)\n",
    "\n",
    "    # Combine the upper triangle mask with the threshold condition\n",
    "    upper_triangle_mask = np.triu(np.ones(similarity_df.shape), k=1).astype(bool)\n",
    "    combined_mask = upper_triangle_mask & (similarity_df > threshold)\n",
    "    masked_df = similarity_df.where(combined_mask)\n",
    "\n",
    "    # iterate through the dataframe to identify the duplicates and create a list\n",
    "    dups = []\n",
    "    for index,row in masked_df.iterrows():\n",
    "        values = row.loc[~pd.isna(row)].index.tolist()\n",
    "        if (values):\n",
    "            dups.append(values + [index + 1]) # include the original theme that we are comparing to\n",
    "        else:\n",
    "            dups.append([index+1])\n",
    "\n",
    "    print(\"initial pass : \", dups)\n",
    "\n",
    "    # there may be cases where values in the dict are identified as being similar to the same theme, but are not included together in a list, e.g., 0: [10, 14, 21, 1], 11: [21, 12].  I think I want to combine all of these.  Not sure there is an easy way to do this...\n",
    "    dups_iter = dups.copy()\n",
    "    any_overlap = True\n",
    "    while (any_overlap):\n",
    "        any_overlap = False\n",
    "        exclude_indices = []\n",
    "        final_list = []\n",
    "\n",
    "        for i1, v1 in enumerate(dups_iter):\n",
    "            #print(\"one\", i1, v1)\n",
    "            values = []\n",
    "            if (i1 not in exclude_indices):\n",
    "                values = v1\n",
    "                for i2, v2 in enumerate(dups_iter):\n",
    "                    if (i2 > i1 and i2 not in exclude_indices):\n",
    "                        #print(i2, v2)\n",
    "                        # check if there is overlap\n",
    "                        overlap = False\n",
    "                        for v in v1:\n",
    "                            if (v in v2):\n",
    "                                overlap = True\n",
    "                                any_overlap = True\n",
    "                        if (overlap):\n",
    "                            values += v2\n",
    "                            exclude_indices.append(i2)\n",
    "                            #print(i1, i2)\n",
    "\n",
    "            if (values):\n",
    "                final_list.append(list(set(values)))\n",
    "        dups_iter = final_list\n",
    "        #print(\"here\",any_overlap)\n",
    "\n",
    "\n",
    "    print(\"final list : \", dups_iter)\n",
    "    print(\"\")\n",
    "    # now print the themes that are considered duplicates\n",
    "    for i,indices in enumerate(dups_iter):\n",
    "        print(i, indices)\n",
    "        rows = themes_df.loc[themes_df['theme_number'].isin(indices)]\n",
    "        for index, row in rows.iterrows():\n",
    "            print(row['theme'])\n",
    "\n",
    "    return dups_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups_iter = combine_dups(\"M1\", threshold = 0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For OpenAI : https://platform.openai.com/docs/guides/embeddings/embedding-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df, themes_df = get_themes(\"M5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
